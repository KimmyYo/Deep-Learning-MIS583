{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Z1Snuk7rIK"
      },
      "source": [
        "# MIS 583 Assignment 6: Text Sentiment Classification with Prompt Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwr9MgZogRZ"
      },
      "source": [
        "Before we start, please put your name and SID in following format: <br>\n",
        ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷, M114020035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DzsjuDhlz_k"
      },
      "source": [
        "**Your Answer:**   \n",
        "Hi I'm 游雅淇, B104020012"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-Zzebq7rIM"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc9gd_Wk7rIN"
      },
      "source": [
        "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive or negative based on the emotions customers express within them.\n",
        "\n",
        "In Task 1, you need to fine-tune a pre-trained language model (e.g., BERT) to predict the sentiment of given tweets.\n",
        "\n",
        "In Task 2, we employ prompts to enable the model to perform sentiment analysis through in-context learning, eliminating the need for additional training.\n",
        "\n",
        "In Task 3, you will use the method called LM-BFF to utilize the model in generating the optimal template and verbalizer autonomously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXeCBB2sxrd8"
      },
      "source": [
        "# Notice\n",
        "**You are not allow to use the model like GPT family or pre-trained weight using SST-2 and twitter dataset!!!!!!!!!!!!!!!!!**\n",
        "\n",
        "You can use BERT and RoBERTa encoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs",
        "tags": []
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.  \n",
        "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
        "This is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vuw-gNvjqcYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea70196f-1ff9-4493-8e9b-3b4368a04c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "torch 2.1.0+cu121\n",
            "torchvision 0.16.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyVEea1oxrd9"
      },
      "source": [
        "# Task 1: Text Sentiment Classification (40 points)\n",
        "\n",
        "In this task, you need to fine-tune a pre-trained language model (e.g., BERT or RoBERTa encoder) to predict the sentiment of given tweets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4s_a5D7rIR"
      },
      "source": [
        "## Loading Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPUkTbnL7rIR"
      },
      "source": [
        "First, let's talk about the model. The Hugging Face team has created an amazing framework called \"transformers\" for NLP tasks. It includes many state-of-the-art machine learning models for PyTorch, TensorFlow, and JAX.\n",
        "\n",
        "To start with this package, follow [this link to installation and a basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK0ouXa09pDU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e25c8f0-5d96-4ad8-fa19-09a9668e23e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happy installation\n",
            "pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (1.60.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.1)\n",
            "Collecting protobuf==3.9.2\n",
            "  Using cached protobuf-3.9.2-py2.py3-none-any.whl (431 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from protobuf==3.9.2) (67.7.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from protobuf==3.9.2) (1.16.0)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.1\n",
            "    Uninstalling protobuf-4.25.1:\n",
            "      Successfully uninstalled protobuf-4.25.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-aiplatform 1.38.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-bigquery 3.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-iam 2.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-resource-manager 1.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "googleapis-common-protos 1.62.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.9.2 which is incompatible.\n",
            "grpcio-status 1.48.2 requires protobuf>=3.12.0, but you have protobuf 3.9.2 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorflow-hub 0.15.0 requires protobuf>=3.19.6, but you have protobuf 3.9.2 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.9.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.10/dist-packages (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.4 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.4)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.4->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.4->boto3) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# you might need some additional installations there\n",
        "!echo happy installation\n",
        "!pip -V\n",
        "!pip install grpcio\n",
        "!pip install google-auth\n",
        "!pip install protobuf==3.9.2\n",
        "!pip install pyprind\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmGCAevi7rIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4982924f-9a15-4733-b90d-0eb2a5ffc727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "#########################################################################\n",
        "#            Loading tokenizer and model from transformer               #\n",
        "#########################################################################\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "\n",
        "# ---------- 1. load from torch.hub ----------\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', bert_type)\n",
        "\n",
        "# ---------- 2. load from installed huggingface ----------\n",
        "tokenizer = BertTokenizerFast.from_pretrained(bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = BertForSequenceClassification.from_pretrained(bert_type)\n",
        "\n",
        "\n",
        "\n",
        "# finetune from the output from bert to your task\n",
        "model.classifier = nn.Linear(768, 3, bias=True)\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "## How to Get Data\n",
        "\n",
        "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/19Ty2lVAm55VL5QIM-MMQhhOzWXeMtxeV/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZnFgi5i_2oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ea060c-4a3e-498f-a8d6-f3eb525d0d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有三個csv檔。\n",
        "\n",
        "- `train.csv`, `test.csv` and `val.csv`\n",
        "\n",
        "Training set 有 **10248** 筆資料.  \n",
        "Validation set 有 **1317** 筆資料.  \n",
        "Testing set 有 **3075** 筆資料.  \n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSlTMdxf8Zd7"
      },
      "outputs": [],
      "source": [
        "!unzip -qq ./drive/MyDrive/DeepLearning/A6/twitter_sentiment.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw5aI3Zaxrd-"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf5GXTme7rIT"
      },
      "outputs": [],
      "source": [
        "# Utility function to extract text and label from csv file\n",
        "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
        "    text_list = []\n",
        "    label_list = []\n",
        "\n",
        "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
        "    with open(f_path) as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for line in reader:\n",
        "            text_list.append(line['text'])\n",
        "            if mode != 'test':\n",
        "                label_list.append(int(line['sentiment_label']))\n",
        "\n",
        "    return text_list, label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fpY0ZrK7rIV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
        "        self.mode = mode\n",
        "\n",
        "        text_list, label_list = get_texts(f_name, mode)\n",
        "        print('mode', mode, 'has', len(text_list), 'datas')\n",
        "        text_list = tokenizer(text_list,\n",
        "                             truncation=True, padding=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "        self.text_list = text_list['input_ids']\n",
        "        self.mask_list = text_list['attention_mask']\n",
        "\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        mask = self.mask_list[idx]\n",
        "        if self.mode == 'test':\n",
        "            return text, mask\n",
        "        label = torch.tensor(self.label_list[idx])\n",
        "        return text, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cm5Fsvoxrd-"
      },
      "source": [
        "## `DataLoader`\n",
        "\n",
        "`torch.utils.data.DataLoader` define how to sample from `dataset` and some other function like:\n",
        "+ `shuffle` : set to `True` to have the data reshuffled at every epoch\n",
        "+ `batch_size` : how many samples per batch to load\n",
        "\n",
        "See [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for more details"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DeepLearning/A6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhs7AylPZed",
        "outputId": "62447b1d-3ed6-4163-c607-f86f0e6ce360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DeepLearning/A6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCmM4FSw7rIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7a995e-0b7f-48f2-b154-bfb3cbdd112f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mode train has 10248 datas\n",
            "mode val has 1317 datas\n",
            "mode test has 3075 datas\n"
          ]
        }
      ],
      "source": [
        "dataset_train = TwitterDataset(mode='train')\n",
        "dataset_val = TwitterDataset(mode='val')\n",
        "dataset_test = TwitterDataset(mode='test')\n",
        "\n",
        "batch_size = 64\n",
        "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n",
        "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
        "                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqkvofHc7rIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd355873-215c-4cea-de2b-1f8df8ea6f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token ['[CLS]', '@', 'united', 'i', 'have', 'never', 'been', 'mis', '##lea', '##d', 'by', 'a', 'company', 'as', 'many', 'times', 'as', 'i', 'have', 'this', 'week', 'by', 'united', 'airlines', '!', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "token to s [CLS] @ united i have never been mislead by a company as many times as i have this week by united airlines! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0]) # converts a sequence of numeric IDs in the training dataset into their corresponding tokens using the specified tokenizer.\n",
        "print('token', t)\n",
        "print('token to s', tokenizer.convert_tokens_to_string(t)) # converts a sequence of tokens (t) back into the original text string using the specified tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlfj1dFbxrd-"
      },
      "source": [
        "# Define loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxZrfCqW7rIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f0d720-eb15-46ec-c9d1-54f40c0a930c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "from torch import nn\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "DWgqiC-RYA7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f531b04f-8833-48ed-a1ad-4f99a29fb121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpwgE2Gd7rIZ"
      },
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlaiAZAD7rIa"
      },
      "outputs": [],
      "source": [
        "def accuracy(raw_preds, y):\n",
        "    preds = raw_preds.argmax(dim=1)\n",
        "    acc = (preds == y).sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r550EyePxrd-"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmc_Gms97rIa"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Testing process                              #\n",
        "        #########################################################################\n",
        "        # 1. Clean the gradients of optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # 2. Put correct variables into model\n",
        "        outputs = model(text, mask)\n",
        "        # 3. Get prediction\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "        loss = criterion(outputs.logits, label)\n",
        "        acc = accuracy(outputs.logits, label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "def test(model, data, criterion, log_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Training process                             #\n",
        "        #########################################################################\n",
        "        # 1. Put correct variables into model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(text, mask)\n",
        "        # 2. Get prediction\n",
        "        # 3. Evalutate by criterion and accuracy\n",
        "        loss = criterion(outputs.logits, label)\n",
        "        acc = accuracy(outputs.logits, label)\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if log_loss:\n",
        "            val_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "# class for monitoring train and test acc/loss\n",
        "class Meter:\n",
        "    def __init__(self):\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.train_acc_list.append(train_acc)\n",
        "        self.val_loss_list.append(val_loss)\n",
        "        self.val_acc_list.append(val_acc)\n",
        "\n",
        "    def plot(self):\n",
        "        x = range(len(self.train_loss_list))\n",
        "        plt.plot(x, self.train_loss_list)\n",
        "        plt.plot(x, self.val_loss_list, color='r')\n",
        "        plt.legend(['train_loss', 'val_loss'])\n",
        "        plt.show()\n",
        "        plt.plot(x, self.train_acc_list)\n",
        "        plt.plot(x, self.val_acc_list, color='r')\n",
        "        plt.legend(['train_acc', 'val_acc'])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZyrKd57rIb"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVDe-fRe7rIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69f0d00-f327-423c-f54c-e6b01bb1d65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:46<00:00,  1.52it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 train_loss: 0.010577772350677934 train_acc: 0.711943793911007\n",
            "Epoch 1 val_loss:  0.013841058597839866 val_acc : 0.8420652999240699\n",
            "---------- e 1 save best model ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 train_loss: 0.00592626416059698 train_acc: 0.8537275565964091\n",
            "Epoch 2 val_loss:  0.013291807351640978 val_acc : 0.8428246013667426\n",
            "---------- e 2 save best model ----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 train_loss: 0.004228772981576534 train_acc: 0.9045667447306791\n",
            "Epoch 3 val_loss:  0.014311166637171916 val_acc : 0.8375094912680334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 train_loss: 0.0028719041946545466 train_acc: 0.9381342701014832\n",
            "Epoch 4 val_loss:  0.016361935323722314 val_acc : 0.8344722854973424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 train_loss: 0.0017717421103187435 train_acc: 0.9648711943793911\n",
            "Epoch 5 val_loss:  0.018173150620624797 val_acc : 0.8367501898253606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 train_loss: 0.0013377987901454066 train_acc: 0.975312256049961\n",
            "Epoch 6 val_loss:  0.018626492740370718 val_acc : 0.8420652999240699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 train_loss: 0.000942068383092923 train_acc: 0.9829234972677595\n",
            "Epoch 7 val_loss:  0.021132753558927863 val_acc : 0.8283978739559605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 train_loss: 0.0007408085530173036 train_acc: 0.985655737704918\n",
            "Epoch 8 val_loss:  0.022966990904536397 val_acc : 0.8215641609719059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 train_loss: 0.0006389422344453168 train_acc: 0.9879976580796253\n",
            "Epoch 9 val_loss:  0.023365410036082473 val_acc : 0.8306757782839788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 161/161 [01:48<00:00,  1.49it/s]\n",
            "100%|██████████| 42/42 [00:04<00:00,  8.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 train_loss: 0.0005801083136047836 train_acc: 0.9882903981264637\n",
            "Epoch 10 val_loss:  0.023423517845797683 val_acc : 0.8367501898253606\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#                          Hyper-parameters                             #\n",
        "#########################################################################\n",
        "max_epoch = 10\n",
        "log_interval = 1\n",
        "best_acc = 0\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################\n",
        "\n",
        "m = Meter()\n",
        "\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
        "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
        "            epoch, train_loss, train_acc\n",
        "        ))\n",
        "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
        "            epoch, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "    # model checkpoint\n",
        "    torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))\n",
        "    if val_acc > best_acc:\n",
        "        best_model = model\n",
        "        best_acc = val_acc\n",
        "        print('-'*10, 'e', epoch, 'save best model', '-'*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmtW58OR7rIc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        },
        "outputId": "bef59715-a86f-4a7b-8920-9f30426f5c2e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot them out\n",
        "m.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcJcHf7n7rId"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/f072e95f51bc48978225941dba218241).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf5UTlMZ7rId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b696c025-85f1-4926-951d-e592d5aa1390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [00:09<00:00,  9.88it/s]\n"
          ]
        }
      ],
      "source": [
        "best_model.eval()\n",
        "\n",
        "total_out = []\n",
        "for text, mask in tqdm(test_data, total=len(test_data)):\n",
        "    text = text.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    output = best_model(text, mask)\n",
        "    pred = output.logits\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    total_out.append(pred)\n",
        "\n",
        "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(total_out):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_iw3-46xrd_"
      },
      "source": [
        "# Task 2: In-Context learning (32 points)\n",
        "\n",
        "In this task, you will learn how to perform sentiment classification using **prompts** without the need for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Q6U8Daxrd_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pyprind\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from transformers import BertConfig, BertTokenizer, BertForMaskedLM\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4wFQSLPxrd_"
      },
      "source": [
        "# Loading model and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5S1Tz-4xrd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc13b938-955a-42a0-b81b-41740664e899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#########################################################################\n",
        "#         TODO: Design your own template(prefix) and verbalizer         #\n",
        "#########################################################################\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "\n",
        "         # self.prefix = \"This emotion sentence is [MASK].\"# you can modify this line\n",
        "        # self.prefix = (\n",
        "        #     \"Classify sentiment: It was a negative emotion sentence. @united never fails, flying FC order ravioli get chicken. Tell the FA she says you should of told me, other people wanted chicken..idiot.\"\n",
        "        #     \"Classify sentiment: It was a positive emotion sentence. @AmericanAir SFO. Natt (the agent who helped me) really did an awesome job. \"\n",
        "        #     \"Classify sentiment: It was a neurtal emotion sentence. @SouthwestAir for my travel I see bz class and get away are sld out. Anytime is available . How do I know how many anytime are available?\"\n",
        "        #     \"Classify sentiment: It was a [MASK] emotion sententce.\"\n",
        "        # )\n",
        "        self.prefix = \"The emotion sentence is classify to [MASK].\"\n",
        "        self.verbalizer = {\n",
        "            'Positive': 2,\n",
        "            'Neurtal': 1,\n",
        "            'Negative': 0\n",
        "        }\n",
        "\n",
        "        self.max_seq_length = 512\n",
        "        self.batch_size = 64\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "bert_type = 'bert-base-uncased'\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(bert_type, num_labels = 3)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "bert_config = BertConfig.from_pretrained(bert_type)\n",
        "\n",
        "bert = model.from_pretrained(bert_type, config=bert_config).to(device)\n",
        "\n",
        "#######################################################################\n",
        "#                        End of your code                             #\n",
        "#######################################################################\n",
        "\n",
        "softmax = nn.Softmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezjSssj7xrd_"
      },
      "source": [
        "## Obtaion verbalizer ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky33or52xrd_"
      },
      "outputs": [],
      "source": [
        "# Utility function to obtaion verbalizer ids\n",
        "def obtain_verbalizer_ids(verbalizer, tokenizer):\n",
        "    verbalizer_ids = tokenizer.convert_tokens_to_ids(list(verbalizer.keys()))\n",
        "    index2ids = {i: verbalizer_ids[i] for i in range(len(verbalizer_ids))}\n",
        "    return verbalizer_ids, index2ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpDyM3I6xrd_"
      },
      "outputs": [],
      "source": [
        "verbalizer_ids, index2ids = obtain_verbalizer_ids(config.verbalizer, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906ZrhBaxrd_"
      },
      "source": [
        "## Concatenate original text and prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj227zdWxrd_"
      },
      "outputs": [],
      "source": [
        "# Utility function to concatenate prefix and text\n",
        "def concatenate_prefix(texts, config):\n",
        "    ##################################################\n",
        "    #   TODO: concatenate your own prefix and text   #\n",
        "    ##################################################\n",
        "    prefix_texts = []\n",
        "    prefix_texts = [f\"{config.prefix} {text}.\" for text in texts]\n",
        "    ##################################################\n",
        "    #                 End of your code               #\n",
        "    ##################################################\n",
        "    return prefix_texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./drive/MyDrive/DeepLearning/A6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiDXJNDjpEs6",
        "outputId": "785222e8-6657-45ec-a75e-34eaaa46c480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './drive/MyDrive/DeepLearning/A6'\n",
            "/content/drive/MyDrive/DeepLearning/A6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUZSYG6Mxrd_"
      },
      "outputs": [],
      "source": [
        "def load_data(config):\n",
        "    # ['texts', 'labels']\n",
        "    df = pd.read_csv('./twitter_sentiment/train.csv')\n",
        "    original_texts = df['text'].tolist()\n",
        "    labels = df['sentiment_label'].tolist()\n",
        "\n",
        "    texts = concatenate_prefix(original_texts, config)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "texts, labels = load_data(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8HInF0Kxrd_"
      },
      "outputs": [],
      "source": [
        "# Batching of texts and labels for training or processing in batches\n",
        "def pack_batch(texts, labels, batch_size):\n",
        "    \"\"\"\n",
        "    :param texts: list\n",
        "    :param labels: list\n",
        "    :param batch_size: int\n",
        "    :return batch_X: list\n",
        "            [[text11, text12, ...], [text21, text22, ...], ...]\n",
        "    :return batch_y: list\n",
        "            [[label11, label12, ...], [label21, label22, ...], ...]\n",
        "    :return batch_count: int\n",
        "    \"\"\"\n",
        "    assert len(texts) == len(labels)\n",
        "\n",
        "    if len(texts) % batch_size != 0:\n",
        "        flag = False\n",
        "        batch_count = int(len(texts) / batch_size) + 1\n",
        "    else:\n",
        "        flag = True\n",
        "        batch_count = int(len(texts) / batch_size)\n",
        "\n",
        "    batch_X, batch_y = [], []\n",
        "\n",
        "    if flag:\n",
        "        for i in range(batch_count):\n",
        "            batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "            batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "    else:\n",
        "        for i in range(batch_count):\n",
        "            if i == batch_count - 1:\n",
        "                batch_X.append(texts[i * batch_size:])\n",
        "                batch_y.append(labels[i * batch_size:])\n",
        "            else:\n",
        "                batch_X.append(texts[i * batch_size: (i + 1) * batch_size])\n",
        "                batch_y.append(labels[i * batch_size: (i + 1) * batch_size])\n",
        "\n",
        "    return batch_X, batch_y, batch_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxEFxm6vxreA"
      },
      "outputs": [],
      "source": [
        "batch_X, batch_y, batch_count = pack_batch(texts, labels, config.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reBPw1TixreA"
      },
      "source": [
        "## Inferencing the model without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOzdXCdIxreA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b435794-ba36-487f-8d05-5468af7e2e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[ 97 %] Time elapsed: 00:06:53 | ETA: 00:00:10"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    predict_all = np.array([], dtype=int)\n",
        "    labels_all = np.array([], dtype=int)\n",
        "    pper = pyprind.ProgPercent(batch_count)\n",
        "    for i in range(batch_count):\n",
        "        inputs = batch_X[i]\n",
        "        labels = batch_y[i]\n",
        "\n",
        "        # Using the BERT tokenizer (tokenizer.batch_encode_plus), adding special tokens, ensuring a maximum sequence length, and handling padding/truncation\n",
        "        tokens = tokenizer.batch_encode_plus(inputs, add_special_tokens=True,\n",
        "                                             max_length=config.max_seq_length,\n",
        "                                             padding='max_length', truncation=True)\n",
        "\n",
        "        ids = torch.tensor(tokens['input_ids']).to(device)\n",
        "        attention_mask = torch.tensor(tokens['attention_mask']).to(device)\n",
        "\n",
        "        # Shape: (batch_size, max_seq_length, vocab_size)\n",
        "        logits = bert(ids, attention_mask=attention_mask).logits\n",
        "\n",
        "        mask_token_index = (ids == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "        # Find [MASK] logits\n",
        "        # shape: (batch_size, vocab_size)\n",
        "        masked_logits = logits[mask_token_index[0], mask_token_index[1], :]\n",
        "\n",
        "        # Extract the logits of the word in the verbalizer at the [MASK] position\n",
        "        # shape: (batch_size, verbalizer_size)\n",
        "        verbalizer_logits = masked_logits[:, verbalizer_ids]\n",
        "\n",
        "        # Construct a pseudo-distribution from the logits in these verbalizers\n",
        "        pseudo_distribution = softmax(verbalizer_logits)\n",
        "\n",
        "        #################################################################################\n",
        "        #   1. Find the index with the maximum probability in the pseudo-distribution   #\n",
        "        #   2. Convert the index to the corresponding word ID                           #\n",
        "        #   3. Convert the ID to a token                                                #\n",
        "        #   4. Find the label corresponding to the token                                #\n",
        "        #################################################################################\n",
        "\n",
        "        pred_indices = torch.argmax(pseudo_distribution, dim=1).cpu().numpy()\n",
        "\n",
        "        pred_ids = [verbalizer_ids[index] for index in pred_indices]\n",
        "\n",
        "        pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids)\n",
        "\n",
        "        pred_labels = [config.verbalizer[token] if token in config.verbalizer else -1 for token in pred_tokens]\n",
        "\n",
        "        #################################################################################\n",
        "        #                             End of your code                                  #\n",
        "        #################################################################################\n",
        "\n",
        "        predict_all = np.append(predict_all, pred_labels)\n",
        "        labels_all = np.append(labels_all, labels)\n",
        "\n",
        "        pper.update()\n",
        "\n",
        "    acc = accuracy_score(labels_all, predict_all)\n",
        "    p = precision_score(labels_all, predict_all, average=\"weighted\")\n",
        "    r = recall_score(labels_all, predict_all, average=\"weighted\")\n",
        "    f1 = f1_score(labels_all, predict_all, average=\"weighted\")\n",
        "\n",
        "    print('accuracy: %f | precision: %f | recall: %f | f1: %f' % (acc, p, r, f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAP7E2tRxreA"
      },
      "source": [
        "# Task 3: LM-BFF (45 points)\n",
        "\n",
        "https://arxiv.org/pdf/2012.15723.pdf\n",
        "\n",
        "Unlike the previous task, LM-BFF can generate templates and verbalizers automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVLPQ78wxreA"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YpuQUoxxreA"
      },
      "source": [
        "請先到共用雲端硬碟將檔案 `SST-2.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/14MDYFasXU94dUE9DjgfcZE61iTRI2007/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVQnpzwuxreA"
      },
      "source": [
        "# Install openprompt\n",
        "\n",
        "This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline.\n",
        "\n",
        "[OpenPrompt Documentation](https://thunlp.github.io/OpenPrompt/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKyS8o1qxreA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727c2965-a723-4d75-940b-9896c1e8b0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openprompt in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.35.2)\n",
            "Requirement already satisfied: sentencepiece==0.1.96 in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.10/dist-packages (from openprompt) (4.66.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (from openprompt) (2.6.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from openprompt) (3.8.1)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.1.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from openprompt) (0.3.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from openprompt) (2.15.0)\n",
            "Requirement already satisfied: rouge==1.0.0 in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.0.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openprompt) (10.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from openprompt) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->openprompt) (0.4.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->openprompt) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->openprompt) (1.3.2)\n",
            "Collecting protobuf>=3.20 (from tensorboardX->openprompt)\n",
            "  Using cached protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.10.0->openprompt) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->openprompt) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->openprompt) (2023.3.post1)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.9.2\n",
            "    Uninstalling protobuf-3.9.2:\n",
            "      Successfully uninstalled protobuf-3.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openprompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edCgWwQExreA"
      },
      "source": [
        "# Import openprompt package"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX4H5B6J--Kp",
        "outputId": "febab1d8-421a-449e-8b6b-a5a64525a75a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt2nR3KixreA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ed76cd-3891-4f03-85a8-612351e51370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWG0JE91xreA"
      },
      "source": [
        "# Setup cuda and whether to perform automatic generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIjW2qPvxreA"
      },
      "outputs": [],
      "source": [
        "cuda = True\n",
        "auto_t = False # Whether to perform automatic template generation\n",
        "auto_v = True # Whether to perform automatic verbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osfr9iTMxreA"
      },
      "source": [
        "# Load dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/drive/MyDrive/DeepLearning/A6/SST-2.zip"
      ],
      "metadata": {
        "id": "ZiGRN2teyWNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/DeepLearning/A6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-LEoFoOz4S1",
        "outputId": "79306239-892e-4170-e588-3f60b1778049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DeepLearning/A6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHPwUuKcxreA"
      },
      "outputs": [],
      "source": [
        "from openprompt.data_utils.text_classification_dataset import SST2Processor\n",
        "dataset = {}\n",
        "dataset['train'] = SST2Processor().get_train_examples(\"./SST-2/\")\n",
        "dataset['validation'] = SST2Processor().get_dev_examples(\"./SST-2/\")\n",
        "dataset['test'] = SST2Processor().get_test_examples(\"./SST-2/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14bVtpwdxreA"
      },
      "outputs": [],
      "source": [
        "#print('load model...')\n",
        "from openprompt.plms import load_plm\n",
        "\n",
        "# load mlm model for main tasks\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n",
        "\n",
        "# load generation model for template generation\n",
        "template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = load_plm('t5', 't5-large')\n",
        "\n",
        "from openprompt.prompts import ManualVerbalizer, ManualTemplate\n",
        "\n",
        "# if you wish to do automatic label word generation, the verbalizer is not the final verbalizer, and is only used for template generation.\n",
        "verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']]) # Manually generate the verbalizer\n",
        "\n",
        "\n",
        "###################################################################################################################\n",
        "#   TODO: You need to switch LMBFFTemplateGenerationTemplate or ManualTemplate to                                 #\n",
        "#         compare auto generate template and manual generate template                                             #\n",
        "###################################################################################################################\n",
        "from openprompt.prompts.prompt_generator import LMBFFTemplateGenerationTemplate\n",
        "\n",
        "############################################\n",
        "#   LMBFFTemplateGenerationTemplate        #\n",
        "############################################\n",
        "import random\n",
        "\n",
        "# number of demonstrations\n",
        "if auto_t:\n",
        "    num_demonstrations = 1  # try different number\n",
        "\n",
        "    demonstrations = []\n",
        "\n",
        "\n",
        "    for _ in range(num_demonstrations):\n",
        "        # random choice training set example with label 0\n",
        "        random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
        "\n",
        "        # random choice training set example with label 1\n",
        "        random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
        "\n",
        "        demonstration = f'{random_example_1.text_a} It was awful. {random_example_2.text_a} It was awesome.'\n",
        "        demonstrations.append(demonstration)\n",
        "\n",
        "    # You can modify the demonstrations and try different combinations\n",
        "    template_text = '{\"placeholder\": \"text_a\"} {\"mask\"} {\"meta\": \"labelword\"} {\"mask\"}.' + ' '.join(demonstrations)\n",
        "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "\n",
        "#############################################\n",
        "#   End of LMBFFTemplateGenerationTemplate  #\n",
        "#############################################\n",
        "\n",
        "########################################\n",
        "#          ManualTemplate              #\n",
        "########################################\n",
        "else:\n",
        "  num_demonstrations = 1\n",
        "  demonstrations = []\n",
        "\n",
        "  for _ in range(num_demonstrations):\n",
        "      # random choice training set example with label 0\n",
        "      random_example_1 = random.choice([example for example in dataset['train'] if example.label == 0])\n",
        "\n",
        "      # random choice training set example with label 1\n",
        "      random_example_2 = random.choice([example for example in dataset['train'] if example.label == 1])\n",
        "\n",
        "      demonstration = f'{random_example_1.text_a} Classify sentiment: This is awesome.  {random_example_2.text_a} Classify sentiment: This is awful.'\n",
        "      demonstrations.append(demonstration)\n",
        "\n",
        "  # Template 1: It was terrible / great (try to change it back to easier version )\n",
        "  # Template 2: The tweet tells the {\"mask\"} sentiment. -> 0.48 (awful QQ)\n",
        "  # Template 3: Classify the sentiment of the message: It is a {\"mask\"}\n",
        "  # Template 4: Classify sentiment: It was {\"mask\"}\n",
        "  # Awful - Awesome\n",
        "  template_text = '{\"placeholder\":\"text_a\"}  Classify sentiment:  {\"mask\"} .' + ' '.join(demonstrations)\n",
        "  template = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
        "  verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['aweful'],['awesome']])\n",
        "\n",
        "########################################\n",
        "#          End of ManualTemplate       #\n",
        "########################################\n",
        "\n",
        "###################################################################################################################\n",
        "#                                           End of your code                                                      #\n",
        "###################################################################################################################\n",
        "\n",
        "\n",
        "# view wrapped example\n",
        "wrapped_example = template.wrap_one_example(dataset['train'][0])\n",
        "print(\"dataset:\", dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIHukTjGxreA"
      },
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-s6hZl6xreB"
      },
      "outputs": [],
      "source": [
        "from openprompt.plms import load_plm\n",
        "from openprompt.prompts.prompt_generator import T5TemplateGenerator\n",
        "from openprompt.pipeline_base import PromptDataLoader, PromptForClassification\n",
        "from openprompt.prompts import ManualTemplate\n",
        "from openprompt.trainer import ClassificationRunner\n",
        "import copy\n",
        "import torch\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "\n",
        "# Returns the best evaluation score achieved during training\n",
        "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n",
        "    best_score = 0.0\n",
        "    for epoch in range(5):\n",
        "        train_loss = train_epoch(model, train_dataloader, loss_func, optimizer)\n",
        "        score = evaluate(model, val_dataloader)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "        print(f\"Epoch {epoch+1}: Train loss={train_loss}, Eval score={score}\")\n",
        "    return best_score\n",
        "\n",
        "# Trains the model on the training data and computes the training loss\n",
        "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
        "    model.train()\n",
        "    loss_all = []\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        #####################################################\n",
        "        # 1. Put correct variables into model to get logits #\n",
        "        # 2. Get labels                                     #\n",
        "        # 3. Evalutate using loss_func                         #\n",
        "        # 4. Append loss to loss_all                        #\n",
        "        #####################################################\n",
        "        logits = model(batch=inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.backward()\n",
        "        loss_all.append(loss.item())\n",
        "        #####################################################\n",
        "        #                 End of your code                  #\n",
        "        #####################################################\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(loss_all)\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    with torch.no_grad():\n",
        "        for step, inputs in enumerate(val_dataloader):\n",
        "            if cuda:\n",
        "                inputs = inputs.cuda()\n",
        "            #####################################################\n",
        "            # 1. Put correct variables into model to get logits #\n",
        "            # 2. Get labels                                     #\n",
        "            # 3. Extend labels to list                          #\n",
        "            # 4. Get predictions and extend preds to list        #\n",
        "            #####################################################\n",
        "            logits = model(batch=inputs)\n",
        "            labels = inputs['label']\n",
        "            alllabels.extend(labels.cpu().numpy())\n",
        "            allpreds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "            #####################################################\n",
        "            #                 End of your code                  #\n",
        "            #####################################################\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MMFTEpvxreB"
      },
      "source": [
        "# Automatic template generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgoiTl0PxreB"
      },
      "source": [
        "Generated template from TemplateGenerator and find the best template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmVeWMHgxreB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d93e88c-9b27-44bd-fcbd-a188896f5f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "performing auto_t...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 32it [00:00, 704.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:54<00:00,  3.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['{\"placeholder\": \"text_a\"} It was {\"mask\"} ..as lively an account as seinfeld is deadpan . It was awful. you \\'re not merely watching history , you \\'re engulfed by it . It was awesome.', '{\"placeholder\": \"text_a\"} it was {\"mask\"} ..as lively an account as seinfeld is deadpan . It was awful. you \\'re not merely watching history , you \\'re engulfed by it . It was awesome.', '{\"placeholder\": \"text_a\"} A {\"mask\"} film.as lively an account as seinfeld is deadpan . It was awful. you \\'re not merely watching history , you \\'re engulfed by it . It was awesome.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . It was great.as lively an account as seinfeld is deadpan . It was awful. you \\'re not merely watching history , you \\'re engulfed by it . It was awesome.', '{\"placeholder\": \"text_a\"} It was {\"mask\"} . it was great.as lively an account as seinfeld is deadpan . It was awful. you \\'re not merely watching history , you \\'re engulfed by it . It was awesome.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} ..as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" ..as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 405.06it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 471.43it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.3788503396644956, Eval score=0.5\n",
            "Epoch 2: Train loss=0.7209148601396009, Eval score=0.5\n",
            "Epoch 3: Train loss=0.7038224078714848, Eval score=0.5625\n",
            "Epoch 4: Train loss=0.7475416832273822, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [01:36<06:24, 96.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.9873891160823405, Eval score=0.59375\n",
            "current template: {\"placeholder\": \"text_a\"} it was {\"mask\"} ..as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' it was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" ..as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 985.46it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 891.76it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.3662884222898128, Eval score=0.5\n",
            "Epoch 2: Train loss=0.7605830564862117, Eval score=0.84375\n",
            "Epoch 3: Train loss=0.2019781855542533, Eval score=0.71875\n",
            "Epoch 4: Train loss=0.17794147432687168, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [03:09<04:44, 94.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.41671413579206273, Eval score=0.5\n",
            "current template: {\"placeholder\": \"text_a\"} A {\"mask\"} film.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' A', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" film.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 499.59it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 554.56it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.0143360996180204, Eval score=0.75\n",
            "Epoch 2: Train loss=0.5368834123923989, Eval score=0.71875\n",
            "Epoch 3: Train loss=0.674017313940567, Eval score=0.8125\n",
            "Epoch 4: Train loss=0.21578987682369188, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [04:43<03:08, 94.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.012904993138363352, Eval score=0.875\n",
            "current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . It was great.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . It was great.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 891.61it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 936.97it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.7561625321395695, Eval score=0.5\n",
            "Epoch 2: Train loss=0.8468441884033382, Eval score=0.53125\n",
            "Epoch 3: Train loss=1.0427600700058974, Eval score=0.5\n",
            "Epoch 4: Train loss=0.7401762790977955, Eval score=0.53125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [06:16<01:33, 93.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.7574067390523851, Eval score=0.875\n",
            "current template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . it was great.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome., wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . it was great.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 956.20it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 976.92it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.3558963533723727, Eval score=0.53125\n",
            "Epoch 2: Train loss=0.5587274773133686, Eval score=0.9375\n",
            "Epoch 3: Train loss=0.059063952230644645, Eval score=0.90625\n",
            "Epoch 4: Train loss=0.0008127959414423458, Eval score=0.90625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [07:49<00:00, 93.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.00036845131387508445, Eval score=0.90625\n",
            "final best template: {\"placeholder\": \"text_a\"} It was {\"mask\"} . it was great.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\n",
            "wrapped example: [[{'text': 'nothing happens , and it happens to flat characters .', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' It was', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': \" . it was great.as lively an account as seinfeld is deadpan . It was awful. you 're not merely watching history , you 're engulfed by it . It was awesome.\", 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 'train-0', 'label': 0}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class ManualTemplateWithoutParse(ManualTemplate):\n",
        "    \"\"\"The generated template from TemplateGenerator is a list of dict of parsed template_text. So no further parsing is needed.\"\"\"\n",
        "    def on_text_set(self):\n",
        "        pass\n",
        "\n",
        "# Template generation\n",
        "if auto_t:\n",
        "    print('performing auto_t...')\n",
        "\n",
        "    if cuda:\n",
        "        template_generate_model = template_generate_model.cuda()\n",
        "\n",
        "    # Creates an instance of T5TemplateGenerator, used for generating text templates\n",
        "    template_generator = T5TemplateGenerator(template_generate_model, template_generate_tokenizer, template_tokenizer_wrapper, verbalizer, beam_width=5) # Beam_width is set to 5 here for efficiency; to improve performance, try a larger number.\n",
        "\n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=template_generate_tokenizer, tokenizer_wrapper_class=template_tokenizer_wrapper, batch_size=len(dataset['train']), decoder_max_length=128, max_seq_length=128, shuffle=False, teacher_forcing=False) # Register all data at once\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        template_generator._register_buffer(data)\n",
        "\n",
        "    template_generate_model.eval()\n",
        "    print('generating...')\n",
        "    template_texts = template_generator._get_templates() # Calls _get_templates on template_generator to generate template texts.\n",
        "\n",
        "    # Converting and Printing Templates\n",
        "    original_template = template.text\n",
        "    template_texts = [template_generator.convert_template(template_text, original_template) for template_text in template_texts]\n",
        "    # template_generator._show_template()\n",
        "    template_generator.release_memory()\n",
        "    # Generate a number of candidate template text\n",
        "    print(template_texts)\n",
        "\n",
        "    # Iterate over each candidate and select the best one\n",
        "    best_metrics = 0.0\n",
        "    best_template_text = None\n",
        "    for template_text in tqdm(template_texts):\n",
        "        verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "        template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "        print(f\"current template: {template_text}, wrapped example: {template.wrap_one_example(dataset['train'][0])}\")\n",
        "\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "\n",
        "        #######################################################\n",
        "        # TODO: Use score to Find your best template_text     #\n",
        "        #######################################################\n",
        "        if score > best_metrics :\n",
        "            best_metrics = score\n",
        "            best_template_text = template_text\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # Use the best template\n",
        "    verbalizer = ManualVerbalizer(tokenizer=tokenizer, num_classes=2, label_words=[['terrible'],['great']])\n",
        "    template = LMBFFTemplateGenerationTemplate(tokenizer=template_generate_tokenizer, verbalizer=verbalizer, text=template_text)\n",
        "    print(\"final best template:\", best_template_text)\n",
        "    print(\"wrapped example:\", template.wrap_one_example(dataset[\"train\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umVjiSThxreB"
      },
      "source": [
        "# Automatic verbalizer generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw-qFO_4xreB"
      },
      "source": [
        "Verbalizer template from VerbalizerGenerator and find the best verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw6E8LzmxreB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b515b4-c363-4353-ffbe-6da316d58d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "performing auto_v...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 32it [00:00, 256.32it/s]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]\n",
            "tokenizing: 32it [00:00, 568.82it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 564.48it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.7308656173408963, Eval score=0.875\n",
            "Epoch 2: Train loss=0.3342106766795041, Eval score=0.875\n",
            "Epoch 3: Train loss=0.07138200757253799, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.1370800447516558, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/30 [01:34<45:50, 94.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.39307233259023633, Eval score=0.65625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 571.40it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 584.89it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.434628279064782, Eval score=0.875\n",
            "Epoch 2: Train loss=0.3380645815486787, Eval score=0.84375\n",
            "Epoch 3: Train loss=0.3330786572514626, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.02494255677356705, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 2/30 [03:08<43:54, 94.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.004318473111652565, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 923.04it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 914.32it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.8929610189502168, Eval score=0.625\n",
            "Epoch 2: Train loss=0.9853983572684228, Eval score=0.5\n",
            "Epoch 3: Train loss=0.867369195446372, Eval score=0.65625\n",
            "Epoch 4: Train loss=0.4610640139968609, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [04:41<42:08, 93.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.7018242472789102, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 965.28it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 879.83it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.6427864305256463, Eval score=0.5\n",
            "Epoch 2: Train loss=0.5159604388172738, Eval score=0.8125\n",
            "Epoch 3: Train loss=0.46437889635171814, Eval score=0.75\n",
            "Epoch 4: Train loss=0.5576628510607406, Eval score=0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/30 [06:14<40:26, 93.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.23769509304474923, Eval score=0.71875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 574.90it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 627.25it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.7853019943868276, Eval score=0.59375\n",
            "Epoch 2: Train loss=0.9519972576526925, Eval score=0.5\n",
            "Epoch 3: Train loss=0.7045254241675138, Eval score=0.71875\n",
            "Epoch 4: Train loss=0.6249904092401266, Eval score=0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 5/30 [07:47<38:49, 93.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.46443918682234653, Eval score=0.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 566.81it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 586.65it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.072862896951847, Eval score=0.5625\n",
            "Epoch 2: Train loss=0.47002663275634404, Eval score=0.78125\n",
            "Epoch 3: Train loss=0.43007341233715124, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.22548287478639395, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 6/30 [09:20<37:15, 93.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.02067576405931959, Eval score=0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 613.33it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 630.87it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.502819861532771, Eval score=0.84375\n",
            "Epoch 2: Train loss=0.8812451722258743, Eval score=0.5\n",
            "Epoch 3: Train loss=0.8604242359870113, Eval score=0.8125\n",
            "Epoch 4: Train loss=0.3071428047842346, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/30 [10:53<35:42, 93.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.12942861425835872, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 861.79it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 914.16it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.3163142184889125, Eval score=0.90625\n",
            "Epoch 2: Train loss=0.43335610393842217, Eval score=0.875\n",
            "Epoch 3: Train loss=0.3904178712373323, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.31130919674592405, Eval score=0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 8/30 [12:26<34:07, 93.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.015608099643031892, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 897.47it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 973.60it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.268034351593087, Eval score=0.5\n",
            "Epoch 2: Train loss=0.9597488800063729, Eval score=0.5\n",
            "Epoch 3: Train loss=1.5211391863485915, Eval score=0.5\n",
            "Epoch 4: Train loss=0.7215611223655287, Eval score=0.53125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 9/30 [13:59<32:34, 93.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.30213573190849274, Eval score=0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 446.77it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 449.36it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.3375203337855055, Eval score=0.53125\n",
            "Epoch 2: Train loss=0.778754698621924, Eval score=0.8125\n",
            "Epoch 3: Train loss=0.3077308790321922, Eval score=0.8125\n",
            "Epoch 4: Train loss=0.29794130604477687, Eval score=0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 10/30 [15:33<31:04, 93.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.3196651351731816, Eval score=0.71875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 958.45it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 916.85it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.2653866897162516, Eval score=0.5\n",
            "Epoch 2: Train loss=0.7281524856225587, Eval score=0.8125\n",
            "Epoch 3: Train loss=0.14780651551973278, Eval score=0.78125\n",
            "Epoch 4: Train loss=0.021847576900199783, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 11/30 [17:06<29:31, 93.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.008385709052454615, Eval score=0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 1018.55it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 1009.71it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.868155319375588, Eval score=0.5\n",
            "Epoch 2: Train loss=0.9922446418786421, Eval score=0.625\n",
            "Epoch 3: Train loss=0.525842147617368, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.2039537163254863, Eval score=0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 12/30 [18:39<27:59, 93.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.3620210453004802, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 504.88it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 621.61it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.032840773754282, Eval score=0.5\n",
            "Epoch 2: Train loss=0.7120444811880589, Eval score=0.59375\n",
            "Epoch 3: Train loss=0.6942830145962944, Eval score=0.90625\n",
            "Epoch 4: Train loss=0.12957863551855553, Eval score=0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 13/30 [20:13<26:27, 93.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.010807514455791534, Eval score=0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 987.74it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 777.60it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.6624657374045455, Eval score=0.5\n",
            "Epoch 2: Train loss=0.33922819351755606, Eval score=0.65625\n",
            "Epoch 3: Train loss=0.48030682686101045, Eval score=0.5\n",
            "Epoch 4: Train loss=0.26656636017287383, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 14/30 [21:46<24:52, 93.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.24192779287317023, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 870.93it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 879.52it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.5696520069035387, Eval score=0.5\n",
            "Epoch 2: Train loss=1.2285213744617067, Eval score=0.5\n",
            "Epoch 3: Train loss=0.563213175162673, Eval score=0.75\n",
            "Epoch 4: Train loss=0.050665137837199836, Eval score=0.59375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 15/30 [23:19<23:19, 93.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.6302038946338087, Eval score=0.6875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 480.13it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 564.61it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.4374728344751873, Eval score=0.5\n",
            "Epoch 2: Train loss=1.5534925406973343, Eval score=0.4375\n",
            "Epoch 3: Train loss=0.8014679537154734, Eval score=0.5\n",
            "Epoch 4: Train loss=0.8582912464626133, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 16/30 [24:52<21:45, 93.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.7588534550741315, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 949.87it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 1007.46it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.53555009447075, Eval score=0.5\n",
            "Epoch 2: Train loss=1.001117654144764, Eval score=0.5\n",
            "Epoch 3: Train loss=0.6860315389931202, Eval score=0.5\n",
            "Epoch 4: Train loss=0.8017798159271479, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 17/30 [26:25<20:11, 93.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.7925674063153565, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 927.24it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 920.64it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.2658059442183003, Eval score=0.875\n",
            "Epoch 2: Train loss=0.4404232879751362, Eval score=0.75\n",
            "Epoch 3: Train loss=0.37065357723622583, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.09877222162322141, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 18/30 [27:58<18:37, 93.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.2622024608645006, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 546.38it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 544.94it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.8827171617885874, Eval score=0.5\n",
            "Epoch 2: Train loss=0.9640453192405403, Eval score=0.53125\n",
            "Epoch 3: Train loss=1.368772170163055, Eval score=0.5\n",
            "Epoch 4: Train loss=0.8198381632100791, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 19/30 [29:32<17:04, 93.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.7447951924987137, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 905.67it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 918.55it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.5852834931572202, Eval score=0.5\n",
            "Epoch 2: Train loss=0.8073873344401363, Eval score=0.5\n",
            "Epoch 3: Train loss=0.580040626344271, Eval score=0.75\n",
            "Epoch 4: Train loss=0.20817268032897118, Eval score=0.59375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 20/30 [31:04<15:30, 93.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.036154895058757575, Eval score=0.6875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 547.06it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 589.47it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.4807758441821761, Eval score=0.90625\n",
            "Epoch 2: Train loss=0.8792197631846648, Eval score=0.625\n",
            "Epoch 3: Train loss=0.3769124720201944, Eval score=0.9375\n",
            "Epoch 4: Train loss=0.10601338281958306, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 21/30 [32:38<13:58, 93.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.0043554261301324, Eval score=0.90625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 928.22it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 970.32it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.608389416300895, Eval score=0.90625\n",
            "Epoch 2: Train loss=0.7901786317961523, Eval score=0.84375\n",
            "Epoch 3: Train loss=0.28996773151447996, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.10132509288814617, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 22/30 [34:11<12:24, 93.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.015030532123660123, Eval score=0.90625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 927.71it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 931.76it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.494039619108662, Eval score=0.5\n",
            "Epoch 2: Train loss=0.746291107032448, Eval score=0.5\n",
            "Epoch 3: Train loss=0.5494736937798734, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.11936947945287102, Eval score=0.65625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 23/30 [35:44<10:51, 93.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.11575567502268314, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 557.88it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 610.61it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.305606386391446, Eval score=0.5\n",
            "Epoch 2: Train loss=0.7107183420739602, Eval score=0.8125\n",
            "Epoch 3: Train loss=0.5290741748652863, Eval score=0.9375\n",
            "Epoch 4: Train loss=0.07279806985570758, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 24/30 [37:17<09:18, 93.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.0045547049239367254, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 912.88it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 998.01it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.427210847636161, Eval score=0.5\n",
            "Epoch 2: Train loss=1.231894688331522, Eval score=0.5625\n",
            "Epoch 3: Train loss=0.4895266574167181, Eval score=0.84375\n",
            "Epoch 4: Train loss=0.2990022425284735, Eval score=0.84375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 25/30 [38:50<07:45, 93.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.5408657058236486, Eval score=0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 511.98it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 496.09it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.29197664960504, Eval score=0.53125\n",
            "Epoch 2: Train loss=0.7762257028371096, Eval score=0.5625\n",
            "Epoch 3: Train loss=0.5669488386483863, Eval score=0.5625\n",
            "Epoch 4: Train loss=0.48617300303442335, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 26/30 [40:23<06:12, 93.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.18303347964011607, Eval score=0.59375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 1007.01it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 928.00it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.975980084324874, Eval score=0.5\n",
            "Epoch 2: Train loss=0.9030951374443248, Eval score=0.75\n",
            "Epoch 3: Train loss=0.921239295596024, Eval score=0.53125\n",
            "Epoch 4: Train loss=0.7739874662365764, Eval score=0.59375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 27/30 [41:56<04:39, 93.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.6376506630331278, Eval score=0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 801.64it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 934.39it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.7670635919397064, Eval score=0.5\n",
            "Epoch 2: Train loss=1.530751606231206, Eval score=0.5\n",
            "Epoch 3: Train loss=0.9011580450460315, Eval score=0.5625\n",
            "Epoch 4: Train loss=1.0613687403965741, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 28/30 [43:30<03:06, 93.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=1.0182906913687475, Eval score=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 840.99it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 924.93it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.300379943488224, Eval score=0.8125\n",
            "Epoch 2: Train loss=0.627804022629789, Eval score=0.8125\n",
            "Epoch 3: Train loss=0.3021319625520391, Eval score=0.90625\n",
            "Epoch 4: Train loss=0.1488003215585536, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 29/30 [45:03<01:33, 93.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.01003011354185901, Eval score=0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "tokenizing: 32it [00:00, 913.43it/s]\n",
            "\n",
            "tokenizing: 32it [00:00, 918.33it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=2.10005693544565, Eval score=0.5\n",
            "Epoch 2: Train loss=0.5232211776310578, Eval score=0.75\n",
            "Epoch 3: Train loss=0.10024801531301364, Eval score=0.75\n",
            "Epoch 4: Train loss=0.015656194909652754, Eval score=0.78125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [46:36<00:00, 93.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train loss=0.0003926066138326689, Eval score=0.8125\n",
            "final best label words: ['incredible', 'terrific']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Verbalizer generation\n",
        "from openprompt.prompts.prompt_generator import RobertaVerbalizerGenerator\n",
        "if auto_v:\n",
        "    print('performing auto_v...')\n",
        "    # Load generation model for verbalizer generation\n",
        "    if cuda:\n",
        "        plm = plm.cuda()\n",
        "\n",
        "    # Creates an instance of RobertaVerbalizerGenerator, used for generating verbalizer.\n",
        "    verbalizer_generator = RobertaVerbalizerGenerator(model=plm, tokenizer=tokenizer, candidate_num=20, label_word_num_per_class=20) # To improve performance, try larger numbers\n",
        "\n",
        "\n",
        "    dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, batch_size=32)\n",
        "    for data in dataloader:\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        verbalizer_generator.register_buffer(data)\n",
        "\n",
        "    # Calls generate on verbalizer_generator to generate label words.\n",
        "    label_words_list = verbalizer_generator.generate()\n",
        "    verbalizer_generator.release_memory()\n",
        "\n",
        "    # Iterate over each candidate and select the best one\n",
        "    current_verbalizer = copy.deepcopy(verbalizer)\n",
        "    best_metrics = 0.0\n",
        "    best_label_words = None\n",
        "    for label_words in tqdm(label_words_list):\n",
        "        current_verbalizer.label_words = label_words\n",
        "        train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "        valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "        model = PromptForClassification(copy.deepcopy(plm), template, current_verbalizer)\n",
        "\n",
        "        loss_func = torch.nn.CrossEntropyLoss()\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        # it's always good practice to set no decay to bias and LayerNorm parameters\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)\n",
        "\n",
        "        #######################################################\n",
        "        # TODO: Use score to find your best_label_word        #\n",
        "        #######################################################\n",
        "        if score > best_metrics:\n",
        "            best_metrics = score\n",
        "            best_label_words = label_words\n",
        "        #######################################################\n",
        "        #                 End of your code                    #\n",
        "        #######################################################\n",
        "    # use the best verbalizer\n",
        "    print(\"final best label words:\", best_label_words)\n",
        "    verbalizer = ManualVerbalizer(tokenizer, num_classes=2, label_words=best_label_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeeSMzjfxreB"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY7C7Wp_xreB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151fa1d3-5791-4d52-d456-3e7ef8e1ca20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 32it [00:00, 1000.05it/s]\n",
            "tokenizing: 32it [00:00, 933.23it/s]\n",
            "tokenizing: 872it [00:01, 512.81it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss=1.0570310565526597, Eval score=0.9375\n",
            "Epoch 2: Train loss=0.11199774296983378, Eval score=0.875\n",
            "Epoch 3: Train loss=0.012060872409676904, Eval score=0.875\n",
            "Epoch 4: Train loss=0.5678141137904049, Eval score=0.65625\n",
            "Epoch 5: Train loss=0.5213084513770809, Eval score=0.75\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = PromptDataLoader(dataset['train'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass, shuffle=True)\n",
        "valid_dataloader = PromptDataLoader(dataset['validation'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "test_dataloader = PromptDataLoader(dataset['test'], template, tokenizer=tokenizer, tokenizer_wrapper_class=WrapperClass)\n",
        "\n",
        "\n",
        "model = PromptForClassification(copy.deepcopy(plm), template, verbalizer)\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "# It's always good practice to set no decay to bias and LayerNorm parameters\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "score = fit(model, train_dataloader, valid_dataloader, loss_func, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKpYnD6qxreB"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Predict the results based on testing set. Upload to [Kaggle](https://www.kaggle.com/t/5b8876ed26fd495b8353ad7ce94b6f65)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyT0kW0qxreB"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "allpreds = []\n",
        "for step, inputs in enumerate(test_dataloader):\n",
        "    if cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = model(inputs)\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(allpreds):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXm2rPR8xreB"
      },
      "source": [
        "# Report (15 points)\n",
        "\n",
        "- Task 1: Compare **two** different models you employed and provide a brief discussion of your implementation.\n",
        "\n",
        "- Task 2: You need to try at least **three** different templates and verbalizers to compare how your prompts work with the model. Report your performance in zero-shot, one-shot, and few-shot scenarios, with examples drawn from the training set.\n",
        "\n",
        "- Task 3: Try at least three different manually crafted templates to compare them with auto-generated templates. Evaluate the performance with different numbers of demonstrations and plot the graph from Figure 3 in the paper (https://arxiv.org/pdf/2012.15723.pdf). Also, report your best template and verbalizer.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "51ee1b965d6f75a20b2b6babb72920dce4fab5775c12eb1659af0fb55d185fed"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}