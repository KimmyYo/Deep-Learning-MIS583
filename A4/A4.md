## Rotation Task Training 
- The rotation predicition accuracy on the test set in the last epochs is 82.82%, whereas the lowest loss is on the training on 42 epoch which is 0.417 and its corresponding accuracy is 83.04%.

The rotation task used the original images and added rotated images as dataset and aim to predict the rotation of the dataset.

**Discussion** <br>
Reason that this task helps in learning features for downstream task said in the paper that learning rotation task could help the model to learn the representative features which model use to identify rotated images angle like human using head and toes charateristic to identify. As a result, further classfication task could use these learned features (particularlly learn more detail rotated features than no rotated transform training) to classify the dataset more accuratly. That is, the rotation is able to learn the higher level that are less dependent on the specific orientation of the objects in the images, and learn the more useful features for downstream task. 


## Fine-tuning on the pre-trained model or on the randomly initialized model
1. Fine-tune on pre-trained model: test accuracy -> 73.53%
2. Fine-tune on randomly initialized model: test accuracy -> 44.07%

(The fine-tuning method adjust the net from orginial task to classification task)

**Discussion** <br>
As the above discussed, the pre-train model is better for learning high-level and important features, and generalizable for downstream task. The accuracy on two different models shows the significant difference of performance that the pre-train rotation task model did help the classification task model learn crucial and representative features leading to higher accruracy. In details, the randomly initialized model somehow might see more invariant features in compared to pre-train model, which the classification task then might mis-classify due to not learning the crucial features. 

## Supervised learning on the pre-trained model or on the randomly initialized model
 1. On pre-trained model: test accuracy -> 84.05%
 2. On randomly initialized model: test accuracy -> 82.02%

**Discussion** <br>
In compare to fine-tuning on both types of models, re-train the whole models on classification task both have higher and better accuracy. In my opinion, it might due to re-training whole set helps the models learn the classification features more particulars which help models to have better performance. And by comparing to using pre-trained model or not, the pre-trained model did have slightly higher accuracy. However, two models not having significant difference might because that these two models both update the parameters that was freezed in the (2) part. 
And it could show that using fine-tuning with two different of initialized way in compared to using re-training method, fine-tuning with pre-trained model could have better performance than fine-tuning with randomly initialized model. Which indicate that the learning of rotation task could help when the models do not re-train again. 

**Fine tuning different numbers of layers**<br>
- layer3 + layer4 + fc layer
    -> accuracy is 72.30% 
- only fc layer
    -> accuracy is around 27.5%
- layer2 + layer3 + layer4 + fc layer
    -> accuracy is 79.59%
    <p>
    [18,   100] loss: 0.464 acc: 83.65 time: 9.06<br>
    [18,   200] loss: 0.471 acc: 83.21 time: 8.30<br>
    [18,   300] loss: 0.464 acc: 83.51 time: 8.61<br>
    TESTING:<br>
    Accuracy of the network on the 10000 test images: 79.51 %<br>
    Average loss on the 10000 test images: 0.602<br>

    [19,   100] loss: 0.467 acc: 83.45 time: 10.29<br>
    [19,   200] loss: 0.468 acc: 83.46 time: 7.07<br>
    [19,   300] loss: 0.460 acc: 83.81 time: 9.83<br>
    TESTING:<br>
    Accuracy of the network on the 10000 test images: 79.41 %<br>
    Average loss on the 10000 test images: 0.604<br>

    [20,   100] loss: 0.452 acc: 84.24 time: 8.94<br>
    [20,   200] loss: 0.463 acc: 83.62 time: 7.88<br>
    [20,   300] loss: 0.471 acc: 83.31 time: 8.88<br>
    TESTING:<br>
    Accuracy of the network on the 10000 test images: 79.59 %<br>
    Average loss on the 10000 test images: 0.605<br>
    Finished Training<br>
    </p>
   

As the three experiments show, the more the training layers, the higher the test accuracy. Reason can be that if only using fc layer to train, the model only adjust itself to classfication task rather than allowing learning certain parameters to update during trainingO(way lower accurracy). However as the third experiment shows, the more the unfreeze layers, the higer the possibility to overfitting. Therefore, freezing layer in fine-tuning methods should also be adjust to the good fit freezed layers in order to obtain higher accuracy in fine-tune training. 



## Conclusion
So as part 2 and part 3 indicates, pre-trained might not have quite a lot advantages than train from scratch. 
However, fine-tuning could help when there's only limited data or limited computation, that is, without training from scratch for classification task, fine-tunig can improve accuracy. 
In addition, in compared to using pre-trained model or randomly initialized model, the experiments result shows that the representative features pre-trained model learned improve the performance of the both training. 